{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e658068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import re\n",
    "import urllib.request, urllib.error\n",
    "import csv\n",
    "\n",
    "findLink = re.compile(r'href=\"(.*?)\"')\n",
    "findTitle = re.compile('title=\"(.*?)\"')\n",
    "findRatingInteger = re.compile('class=\"integer\">([0-9].)')\n",
    "findRatingFraction = re.compile('</i><i class=\"fraction\">([0-9])<')\n",
    "findActors = re.compile('主演：(.*)')\n",
    "findDirector = re.compile(\n",
    "    '<img .*=\"[\\u4e00-\\u9fa5]{0,30}[0-9]?.?[\\u4e00-\\u9fa5]{0,30}? ([\\u4e00-\\u9fa5]{0,40}·?[\\u4e00-\\u9fa5]{0,40})')\n",
    "findIncome = re.compile('>(.*)<')\n",
    "findType = re.compile('> (.*) <')\n",
    "findDuration = re.compile('(...分钟)')\n",
    "findRegion = re.compile('([\\u4e00-\\u9fa5]{0,20})\\n.*\\/')\n",
    "\n",
    "\n",
    "def main():\n",
    "    baseurl = \"https://maoyan.com/board/4?offset=0\"\n",
    "    datalist = getData(baseurl)\n",
    "    savepath = \"./Data1.csv\"\n",
    "    saveData(savepath, datalist)\n",
    "\n",
    "\n",
    "def getData(baseurl):\n",
    "    datalist = []\n",
    "    for i in range(0, 10):\n",
    "        url = baseurl + str(i * 10)\n",
    "        html = askURL(url)\n",
    "        soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "        for item in soup.find_all('div', class_=\"board-item-main\"):\n",
    "            data = []\n",
    "            item = str(item)\n",
    "            tempLink = \"\".join(re.findall(findLink, item))\n",
    "            link = \"https://maoyan.com\" + tempLink\n",
    "            subHtml = askURL(link)\n",
    "\n",
    "            # 1. Title\n",
    "            name = re.findall(findTitle, item)[0]\n",
    "            data.append(name)\n",
    "            # 2. Name of director\n",
    "            director = getDirector(subHtml)\n",
    "            data.append(director)\n",
    "            # 3. Name of actors\n",
    "            actors = re.findall(findActors, item)[0]\n",
    "            data.append(actors)\n",
    "            # 4. Rating\n",
    "            RatingInteger = re.findall(findRatingInteger, item)[0]\n",
    "            RatingFraction = re.findall(findRatingFraction, item)[0]\n",
    "            Rating = RatingInteger + RatingFraction\n",
    "            data.append(Rating)\n",
    "            # 5. Cumulative income\n",
    "            income = getIncome(subHtml)[0]\n",
    "            data.append(income)\n",
    "            # 6. Type\n",
    "            type = getType(subHtml)\n",
    "            data.append(type)\n",
    "            # 7. Duration\n",
    "            duration = getDuration(subHtml)[0]\n",
    "            data.append(duration)\n",
    "            # 8. Region\n",
    "            region = getRegion(subHtml)[0]\n",
    "            data.append(region)\n",
    "            print(data)\n",
    "            datalist.append(data)\n",
    "\n",
    "    return datalist\n",
    "\n",
    "\n",
    "def getDirector(subHtml):\n",
    "    soup = bs4.BeautifulSoup(subHtml, \"html.parser\")\n",
    "    soup1 = str(soup.find_all(class_=\"celebrity-group\")[0])\n",
    "    director = re.findall(findDirector, soup1)\n",
    "    return director[0]\n",
    "\n",
    "\n",
    "def getIncome(subHtml):\n",
    "    try:\n",
    "        soup = bs4.BeautifulSoup(subHtml, \"html.parser\")\n",
    "        soup1 = str(soup.find_all(class_=\"mbox-name\")[2])\n",
    "        income = re.findall(findIncome, soup1)\n",
    "    except:\n",
    "        return ['暂无']\n",
    "    return income\n",
    "\n",
    "\n",
    "def getType(subHtml):\n",
    "    soup = bs4.BeautifulSoup(subHtml, \"html.parser\")\n",
    "    soup1 = str(soup.find_all(class_=\"ellipsis\")[1])\n",
    "    type = re.findall(findType, soup1)\n",
    "    b = \"\"\n",
    "    for i in type:\n",
    "        b = b + i + \",\"\n",
    "\n",
    "    return b[:-1]\n",
    "\n",
    "\n",
    "def getDuration(subHtml):\n",
    "    soup = bs4.BeautifulSoup(subHtml, \"html.parser\")\n",
    "    soup1 = str(soup.find_all(class_=\"ellipsis\")[2])\n",
    "    duration = re.findall(findDuration, soup1)\n",
    "    return duration\n",
    "\n",
    "def getRegion(subHtml):\n",
    "    soup = bs4.BeautifulSoup(subHtml, \"html.parser\")\n",
    "    soup1 = str(soup.find_all(class_=\"ellipsis\")[2])\n",
    "    region = re.findall(findRegion, soup1)\n",
    "    return region\n",
    "\n",
    "def askURL(url):\n",
    "    head = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\",\n",
    "        \"Cookie\": \"__mta=44528155.1632834697349.1634838577241.1634884364843.63; uuid_n_v=v1; uuid=9C44BA80205D11EC8BAB0F3D6466135EFBA27276280541A8952D36ED1CE50D0A; _lxsdk_cuid=17c2c883781c8-069ca8a30543ec-113f6757-13c680-17c2c883781c8; _lxsdk=9C44BA80205D11EC8BAB0F3D6466135EFBA27276280541A8952D36ED1CE50D0A; _csrf=8a41acec0471af6231bc6c81b20ba865bc375601c6b820114c86a89b52aae88d; __mta=44528155.1632834697349.1634795647201.1634795650394.32; Hm_lvt_703e94591e87be68cc8da0da7cbd0be2=1634808962,1634809019,1634837346,1634837351; Hm_lpvt_703e94591e87be68cc8da0da7cbd0be2=1634884365; _lxsdk_s=17ca6f61307-357-0dd-b7||1\",\n",
    "        \"connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://maoyan.com/board/4\",\n",
    "        \"Host\": \"maoyan.com\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9\"\n",
    "\n",
    "    }\n",
    "    request = urllib.request.Request(url, headers=head)\n",
    "    html = \"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode(\"utf-8\")\n",
    "    except urllib.error.URLError as e:\n",
    "        print(\"a\")\n",
    "    return html\n",
    "\n",
    "\n",
    "def saveData(savepath, data):\n",
    "    headers = ['电影名称', '导演', '演员', '评分', '收入', '类型', '时长', '国家']\n",
    "\n",
    "    with open(savepath, 'w')as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(headers)\n",
    "        f_csv.writerows(data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
