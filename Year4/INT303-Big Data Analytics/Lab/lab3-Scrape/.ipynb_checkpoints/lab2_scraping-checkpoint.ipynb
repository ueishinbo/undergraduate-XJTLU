{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  INT303 Big Data Analytics\n",
    "\n",
    "\n",
    "##  Lab 3 Scraping\n",
    "\n",
    "**XJTLU**<br>\n",
    "**S1 2021**<br>\n",
    "**Instructors:** Jia WANG <br>\n",
    "**Lab Instructor:** Jia WANG <br>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh and Pavlos Protopaps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/styles/cs109.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b2e2168b1477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/styles/cs109.css\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-b2e2168b1477>\u001b[0m in \u001b[0;36mcss_styling\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/styles/cs109.css\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/styles/cs109.css'"
     ]
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"/styles/cs109.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn.apionly as sns\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll scrape Goodread's Best Books list:\n",
    "\n",
    "https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1 .\n",
    "\n",
    "We'll walk through scraping the list pages for the book names/urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Exploring the Web pages and downloading them</li>\n",
    "<li> Parse the page, extract book urls </li>\n",
    "<li> Parse a book page, extract book properties </li>\n",
    "<li> Set up a pipeline for fetching and parsing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "Understand the structure of a web page. Use Beautiful soup to scrape content from these web pages.\n",
    "\n",
    "*This lab corresponds to lectures 2, 3 and 4 and maps on to homework 1 and further.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the web pages and downloading them\n",
    "\n",
    "We're going to see the structure of Goodread's best books list. We'll use the Developer tools in chrome, safari and firefox have similar tools available\n",
    "\n",
    "![](images/goodreads1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To getch this page we use the `requests` module. But are we allowed to do this? Lets check:\n",
    "\n",
    "https://www.goodreads.com/robots.txt\n",
    "\n",
    "Yes we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "url = URLSTART+BESTBOOKS+'1'\n",
    "print(url)\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see properties of the page. Most relevant are `status_code` and `text`. The former tells us  if the web-page was found, and if found , ok. (See lecture notes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code # 200 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a loop to fetch 2 pages of \"best-books\" from goodreads. Notice the use of a format string. This is an example of old-style python format strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "for i in range(1,3):\n",
    "    bookpage=str(i)\n",
    "    stuff=requests.get(URLSTART+BESTBOOKS+bookpage)\n",
    "    filetowrite=\"files/page\"+ '%02d' % i + \".html\"\n",
    "    print(\"FTW\", filetowrite)\n",
    "    fd=open(filetowrite,\"w\")\n",
    "    fd.write(stuff.text)\n",
    "    fd.close()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse the page, extract book urls\n",
    "\n",
    "Notice how we do file input-output, and use beautiful soup in the code below. The `with` construct ensures that the file being read is closed, something we do explicitly for the file being written. We look for the elements with class `bookTitle`, extract the urls, and write them into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookdict={}\n",
    "for i in range(1,3):\n",
    "    books=[]\n",
    "    stri = '%02d' % i\n",
    "    filetoread=\"files/page\"+ stri + '.html'\n",
    "    print(\"FTW\", filetoread)\n",
    "    with open(filetoread) as fdr:\n",
    "        data = fdr.read()\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    for e in soup.select('.bookTitle'):\n",
    "        books.append(e['href'])\n",
    "    print(books[:10])\n",
    "    bookdict[stri]=books\n",
    "    fd=open(\"files/list\"+stri+\".txt\",\"w\")\n",
    "    fd.write(\"\\n\".join(books))\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is George Orwell's 1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookdict['02'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lets go look at the first URLs on both pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse a book page, extract book properties\n",
    "\n",
    "Ok so now lets dive in and get one of these these files and parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "furl=URLSTART+bookdict['02'][0]\n",
    "furl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/goodreads3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstuff=requests.get(furl)\n",
    "print(fstuff.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=BeautifulSoup(fstuff.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.select(\"meta[property='og:title']\")[0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get everything we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=BeautifulSoup(fstuff.text, 'html.parser')\n",
    "print(\n",
    "\"title\", d.select_one(\"meta[property='og:title']\")['content'],\"\\n\",\n",
    "\"isbn\", d.select(\"meta[property='books:isbn']\")[0]['content'],\"\\n\",\n",
    "\"type\", d.select(\"meta[property='og:type']\")[0]['content'],\"\\n\",\n",
    "\"author\", d.select(\"meta[property='books:author']\")[0]['content'],\"\\n\",\n",
    "\"average rating\", d.select_one(\"span.average\").text,\"\\n\",\n",
    "\"ratingCount\", d.select(\"meta[itemprop='ratingCount']\")[0][\"content\"],\"\\n\",\n",
    "\"reviewCount\", d.select_one(\"span.count\")[\"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know what to do, lets wrap our fetching into a proper script. So that we dont overwhelm their servers, we will only fetch 5 from each page, but you get the idea...\n",
    "\n",
    "We'll segue of a bit to explore new style format strings. See https://pyformat.info for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"list{:0>2}.txt\".format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"4\"\n",
    "b = 4\n",
    "class Four:\n",
    "    def __str__(self):\n",
    "        return \"Fourteen\"\n",
    "c=Four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The hazy cat jumped over the {} and {} and {}\".format(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up a pipeline for fetching and parsing\n",
    "\n",
    "Ok lets get back to the fetching..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched=[]\n",
    "for i in range(1,3):\n",
    "    with open(\"files/list{:0>2}.txt\".format(i)) as fd:\n",
    "        counter=0\n",
    "        for bookurl_line in fd:\n",
    "            if counter > 4:\n",
    "                break\n",
    "            bookurl=bookurl_line.strip()\n",
    "            stuff=requests.get(URLSTART+bookurl)\n",
    "            filetowrite=bookurl.split('/')[-1]\n",
    "            filetowrite=\"files/\"+str(i)+\"_\"+filetowrite+\".html\"\n",
    "            print(\"FTW\", filetowrite)\n",
    "            fd=open(filetowrite,\"w\", encoding='utf-8')\n",
    "            fd.write(stuff.text)\n",
    "            fd.close()\n",
    "            fetched.append(filetowrite)\n",
    "            time.sleep(2)\n",
    "            counter=counter+1\n",
    "            \n",
    "print(fetched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we are off to parse each one of the html pages we fetched. We have provided the skeleton of the code and the code to parse the year, since it is a bit more complex...see the difference in the screenshots above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "yearre = r'\\d{4}'\n",
    "def get_year(d):\n",
    "    if d.select_one(\"nobr.greyText\"):\n",
    "        return d.select_one(\"nobr.greyText\").text.strip().split()[-1][:-1]\n",
    "    else:\n",
    "        thetext=d.select(\"div#details div.row\")[1].text.strip()\n",
    "        rowmatch=re.findall(yearre, thetext)\n",
    "        if len(rowmatch) > 0:\n",
    "            rowtext=rowmatch[0].strip()\n",
    "        else:\n",
    "            rowtext=\"NA\"\n",
    "        return rowtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Your job is to fill in the code to get the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_genres(d):\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listofdicts=[]\n",
    "for filetoread in fetched:\n",
    "    print(filetoread)\n",
    "    td={}\n",
    "    with open(filetoread) as fd:\n",
    "        datext = fd.read()\n",
    "    d=BeautifulSoup(datext, 'html.parser')\n",
    "    td['title']=d.select_one(\"meta[property='og:title']\")['content']\n",
    "    td['isbn']=d.select_one(\"meta[property='books:isbn']\")['content']\n",
    "    td['booktype']=d.select_one(\"meta[property='og:type']\")['content']\n",
    "    td['author']=d.select_one(\"meta[property='books:author']\")['content']\n",
    "    td['rating']=d.select_one(\"span.average\").text\n",
    "    td['ratingCount']=d.select_one(\"meta[itemprop='ratingCount']\")[\"content\"]\n",
    "    td['reviewCount']=d.select_one(\"span.count\")[\"title\"]\n",
    "    td['year'] = get_year(d)\n",
    "    td['file']=filetoread\n",
    "    glist = get_genres(d)\n",
    "    td['genres']=\"|\".join(glist)\n",
    "    listofdicts.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofdicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets write all this stuff into a csv file which we will use to do analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(listofdicts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"files/meta.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
